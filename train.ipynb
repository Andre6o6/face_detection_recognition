{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34\n",
    "\n",
    "class SomeNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=1, anchors=[]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = resnet34(pretrained=True)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        \n",
    "        # TODO more channels\n",
    "        self.conv1 = nn.Conv2d(512, 512, 3, padding=1, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.conv2 = nn.Conv2d(512, 512, 3, padding=1, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 1024, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.last = nn.Conv2d(1024, (5+num_classes)*len(anchors), 1)\n",
    "        \n",
    "        # TODO: multi-head later\n",
    "        #self.conv_reg1 = nn.Conv2d(256, 5, 3, padding=1)\n",
    "        #self.conv_class1 = nn.Conv2d(256, bottleneck, 1)\n",
    "        #self.bn_class = nn.BatchNorm2d(bottleneck)\n",
    "        #self.conv_class2 = nn.Conv2d(bottleneck, num_classes, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        \n",
    "        #-> 3 640*640\n",
    "        \n",
    "        x = self.encoder.conv1(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        x = self.encoder.relu(x)\n",
    "        x = self.encoder.maxpool(x)\n",
    "        \n",
    "        x = self.encoder.layer1(x) \n",
    "        x = self.encoder.layer2(x)        \n",
    "        x = self.encoder.layer3(x)        \n",
    "        x = self.encoder.layer4(x)\n",
    "        #-> 512 20*20\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        #-> 1024 5*5\n",
    "        \n",
    "        x = self.last(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/train/labels.txt', 'r') as f:\n",
    "    train_meta = json.load(f)\n",
    "with open('./data/test/labels.txt', 'r') as f:\n",
    "    test_meta = json.load(f)\n",
    "with open('./data/subjects.txt', 'r') as f:\n",
    "    subj_meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from FaceDataset import FaceDataset\n",
    "from region_loss import RegionLoss\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainset = FaceDataset(train_meta, None)\n",
    "testset = FaceDataset(test_meta, None)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=2, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=2, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "net = SomeNet(num_classes=285, anchors=[(60,60), (160,160), (240,240)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 640, 640])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in trainloader:\n",
    "    break\n",
    "batch['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "\n",
    "criterion = RegionLoss(num_classes=285, anchors=[(60,60), (160,160), (240,240)], num_anchors=3)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(verbose=True, info_step=100):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    losses = [0 for i in range(4)]\n",
    "    running_losses = [0 for i in range(4)]\n",
    "    \n",
    "    for i, batch in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = net(batch['img'].to(device))\n",
    "        loss, info = criterion(output, batch['target'].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        for j in range(len(losses)):\n",
    "            losses[j] += info[j]\n",
    "            running_losses[j] += info[j]\n",
    "        \n",
    "        if (i+1) % info_step == 0:\n",
    "            print(' [{} - {}],\\ttrain loss: {:.5}'.format(epoch+1, i+1, running_loss/info_step))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for j in range(len(losses)):\n",
    "                running_losses[j] /= info_step\n",
    "            print(' coord loss: {:.5} \\tobj loss: {:.5} \\tclass loss: {:.5} \\tacc: {:.5}'.format(*running_losses))\n",
    "            running_losses = [0 for j in range(4)]\n",
    "            \n",
    "    train_loss /= len(trainset)\n",
    "    \n",
    "    for i in range(len(losses)):\n",
    "        losses[i] /= len(trainset)\n",
    "        \n",
    "    print('\\n [{}], \\ttrain loss: {:.5}'.format(epoch+1, train_loss))\n",
    "    print(' coord loss: {:.5} \\tobj loss: {:.5} \\tclass loss: {:.5} \\tacc: {:.5}'.format(*losses))\n",
    "    print()\n",
    "    return train_loss, losses\n",
    "  \n",
    "\n",
    "def validate():\n",
    "    net.eval()\n",
    "    losses = [0 for i in range(4)]\n",
    "    val_loss = 0.0\n",
    "    for i, batch in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            output = net(batch['img'].to(device))\n",
    "            loss, info  = criterion(output, batch['target'].to(device))\n",
    "        \n",
    "        val_loss += loss.detach().item()\n",
    "        for j in range(len(losses)):\n",
    "            losses[j] += info[j]\n",
    "            \n",
    "    val_loss /= len(testset)\n",
    "    for i in range(len(losses)):\n",
    "        losses[i] /= len(testset)\n",
    "        \n",
    "    print(' [{}], \\tval loss: {:.5}\\n'.format(epoch+1, val_loss))\n",
    "    print(' coord loss: {:.5} \\tobj loss: {:.5} \\tclass loss: {:.5} \\tacc: {:.5}'.format(*losses))\n",
    "    print()\n",
    "    return val_loss, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1 - 80],\ttrain loss: 57.135\n",
      " coord loss: 0.39881 \tobj loss: 0.36042 \tclass loss: 56.376 \tacc: 0.0125\n",
      " [1 - 160],\ttrain loss: 56.848\n",
      " coord loss: 0.19964 \tobj loss: 0.24367 \tclass loss: 56.404 \tacc: 0.0375\n",
      " [1 - 240],\ttrain loss: 56.543\n",
      " coord loss: 0.11432 \tobj loss: 0.18228 \tclass loss: 56.246 \tacc: 0.075\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3c5d236fffbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-48dc19929581>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(verbose, info_step)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrunning_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/andre6o6/aux/anaconda3/envs/nn/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/andre6o6/aux/anaconda3/envs/nn/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/andre6o6/aux/Projects/VKR/FaceDataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mnp_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m#delete alpha if png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_im\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epoch = 1\n",
    "history = []\n",
    "detailed_history = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    train_loss, train_info = train(info_step=80)\n",
    "    val_loss, test_info = validate()\n",
    "    history.append((train_loss, val_loss))\n",
    "    detailed_history.append((train_info, test_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "from utils import non_max_suppression\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=1, shuffle=False)\n",
    "\n",
    "net.eval()\n",
    "acc = 0\n",
    "for batch in testloader:\n",
    "    with torch.no_grad():\n",
    "        output = net(batch['img'].to(device))\n",
    "        \n",
    "        nB,_,nH,nW = output.size()\n",
    "        nC = 285\n",
    "        nA = 3\n",
    "\n",
    "        anchors = criterion.anchors\n",
    "        stride = 1\n",
    "\n",
    "        prediction = output.view(nB, nA, (4+1+nC), nH, nW)         # reshape for convenience\n",
    "        prediction = prediction.permute(0, 1, 3, 4, 2).contiguous()    # Get bbox_attr dimention to be the last\n",
    "\n",
    "        # Get attributes from output tensor\n",
    "        prediction[..., 0] = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "        prediction[..., 1] = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "        prediction[..., 4] = torch.sigmoid(prediction[..., 4])  # Conf\n",
    "        prediction[..., 5:] = torch.sigmoid(prediction[..., 5:])  # Cls distribution\n",
    "\n",
    "        # Calculate offsets for each grid       \n",
    "        grid_x = torch.arange(nW, dtype=torch.float32).repeat(nW, 1).view([1, 1, nH, nW]).to(device)\n",
    "        grid_y = torch.arange(nH, dtype=torch.float32).repeat(nH, 1).t().view([1, 1, nH, nW]).to(device)\n",
    "        scaled_anchors = torch.FloatTensor([(a_w / stride, a_h / stride) for a_w, a_h in anchors]).to(device)\n",
    "        anchor_w = scaled_anchors[:, 0].view((1, nA, 1, 1))\n",
    "        anchor_h = scaled_anchors[:, 1].view((1, nA, 1, 1))\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        prediction[..., 0] = prediction[..., 0] + grid_x\n",
    "        prediction[..., 1] = prediction[..., 1] + grid_y\n",
    "        prediction[..., 2] = torch.exp(prediction[..., 2]) * anchor_w\n",
    "        prediction[..., 3] = torch.exp(prediction[..., 3]) * anchor_h\n",
    "\n",
    "        prediction = prediction.view(nB, nA*nH*nW, 4+1+nC)\n",
    "        label = non_max_suppression(prediction, nC)[0][-1][-1].cpu().item()\n",
    "        true_label = batch['target'][0][-1][-1].item()\n",
    "        \n",
    "        if label==true_label:\n",
    "            acc+=1\n",
    "acc/len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(640),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "filepath = './data/test/1/27-02.jpg'\n",
    "img = Image.open(filepath)\n",
    "tensor = test_transforms(img).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = net(tensor.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nB,_,nH,nW = output.size()\n",
    "nC = 285\n",
    "nA = 3\n",
    "\n",
    "anchors = criterion.anchors\n",
    "stride = 1\n",
    "\n",
    "prediction = output.view(nB, nA, (4+1+nC), nH, nW)         # reshape for convenience\n",
    "prediction = prediction.permute(0, 1, 3, 4, 2).contiguous()    # Get bbox_attr dimention to be the last\n",
    "\n",
    "# Get attributes from output tensor\n",
    "prediction[..., 0] = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "prediction[..., 1] = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "prediction[..., 4] = torch.sigmoid(prediction[..., 4])  # Conf\n",
    "prediction[..., 5:] = torch.sigmoid(prediction[..., 5:])  # Cls distribution\n",
    "\n",
    "# Calculate offsets for each grid       \n",
    "grid_x = torch.arange(nW, dtype=torch.float32).repeat(nW, 1).view([1, 1, nH, nW]).to(device)\n",
    "grid_y = torch.arange(nH, dtype=torch.float32).repeat(nH, 1).t().view([1, 1, nH, nW]).to(device)\n",
    "scaled_anchors = torch.FloatTensor([(a_w / stride, a_h / stride) for a_w, a_h in anchors]).to(device)\n",
    "anchor_w = scaled_anchors[:, 0].view((1, nA, 1, 1))\n",
    "anchor_h = scaled_anchors[:, 1].view((1, nA, 1, 1))\n",
    "\n",
    "# Add offset and scale with anchors\n",
    "prediction[..., 0] = prediction[..., 0] + grid_x\n",
    "prediction[..., 1] = prediction[..., 1] + grid_y\n",
    "prediction[..., 2] = torch.exp(prediction[..., 2]) * anchor_w\n",
    "prediction[..., 3] = torch.exp(prediction[..., 3]) * anchor_h\n",
    "\n",
    "prediction = prediction.view(nB, nA*nH*nW, 4+1+nC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detections = []\n",
    "\n",
    "#for pred in a:  \n",
    "pred = non_max_suppression(prediction, nC)[0]\n",
    "\n",
    "all_detections.append([np.array([]) for _ in range(nC)])\n",
    "\n",
    "pred_boxes = pred[:, :4].cpu().numpy()\n",
    "scores = pred[:, 4].cpu().numpy()\n",
    "pred_labels = pred[:, -1].cpu().numpy()\n",
    "\n",
    "sort_i = np.argsort(scores)\n",
    "pred_labels = pred_labels[sort_i]\n",
    "pred_boxes = pred_boxes[sort_i]\n",
    "\n",
    "for label in range(nC):\n",
    "    all_detections[-1][label] = pred_boxes[pred_labels == label]\n",
    "    \n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "draw_img = transforms.CenterCrop(640)(img)\n",
    "\n",
    "box = pred_boxes[-1]/5*640\n",
    "\n",
    "draw = ImageDraw.Draw(draw_img)\n",
    "draw.rectangle(box)\n",
    "del draw\n",
    "\n",
    "draw_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
