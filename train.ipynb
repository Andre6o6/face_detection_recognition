{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34\n",
    "\n",
    "class SomeNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=1, num_anchors=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "        \n",
    "        self.encoder = resnet34(pretrained=True)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        \n",
    "        self.reg_head = nn.Conv2d(512, (4+1)*num_anchors, 1)\n",
    "        \n",
    "        self.class_head = nn.Sequential(\n",
    "            nn.Conv2d(512, num_anchors*512, 1),\n",
    "            nn.BatchNorm2d(num_anchors*512),    #GroupNorm(num_anchors, num_anchors*512)\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        #-> 3 320*320\n",
    "        \n",
    "        x = self.encoder.conv1(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        x = self.encoder.relu(x)\n",
    "        x = self.encoder.maxpool(x)\n",
    "        \n",
    "        x = self.encoder.layer1(x) \n",
    "        x = self.encoder.layer2(x)        \n",
    "        x = self.encoder.layer3(x)        \n",
    "        x = self.encoder.layer4(x)\n",
    "        #-> 512 10*10\n",
    "\n",
    "        \n",
    "        x = nn.AdaptiveAvgPool2d((5, 5))(x)\n",
    "        \n",
    "        x_reg = self.reg_head(x)\n",
    "        x_class = self.class_head(x)\n",
    "                \n",
    "        return x_reg, x_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, num_classes=1, num_features=512, num_anchors=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.num_anchors = num_anchors  \n",
    "        self.metric_fn = nn.Conv3d(num_features, num_classes, 1)\n",
    "        \n",
    "    def forward(self, x, target=None):\n",
    "      \n",
    "        nB, _, nH, nW = x.shape\n",
    "        nC = self.num_features\n",
    "        nA = self.num_anchors\n",
    "        \n",
    "        x = x.view(nB, nC, nA, nH, nW)\n",
    "        x_class = self.metric_fn(x)\n",
    "        \n",
    "        x_class = x_class.permute(0, 2, 3, 4, 1) #.contiguous() \n",
    "       \n",
    "        return x_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/train/labels.txt', 'r') as f:\n",
    "    train_meta = json.load(f)\n",
    "with open('./data/test/labels.txt', 'r') as f:\n",
    "    test_meta = json.load(f)\n",
    "with open('./data/subjects.txt', 'r') as f:\n",
    "    subj_meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from FaceDataset import FaceDataset\n",
    "from region_loss import RegionLoss\n",
    "from box_transforms import *\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Anchors and classes\n",
    "anchors=[(30,30), (80,80), (120,120)]\n",
    "num_anchors = len(anchors)\n",
    "num_classes = 285\n",
    "\n",
    "#Transforms on dataset\n",
    "box_transform = Compose([\n",
    "    ResizeWithBox(320),\n",
    "    RandomCropWithBox(320)\n",
    "])\n",
    "\n",
    "val_box_transform = Compose([\n",
    "    ResizeWithBox(320),\n",
    "    CenterCropWithBox(320)\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=.1, hue=.05, saturation=.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#Dataset and loader\n",
    "trainset = FaceDataset(train_meta, box_transform=box_transform, img_transform=transform)\n",
    "testset = FaceDataset(test_meta, box_transform = val_box_transform, img_transform=val_transform)\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#Network\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = SomeNet(num_classes=num_classes, num_anchors=num_anchors).to(device)\n",
    "classifier = Classifier(num_classes=num_classes, num_anchors=num_anchors).to(device)\n",
    "#net.load_state_dict(torch.load('/gdrive/My Drive/data/basenet34_smoothl1_320_e25'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 640, 640])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in trainloader:\n",
    "    break\n",
    "batch['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "criterion = RegionLoss(num_classes=num_classes, anchors=anchors, num_anchors=num_anchors)\n",
    "optimizer = torch.optim.Adam([{'params': net.parameters()}, {'params': classifier.parameters()}], lr=lr)\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(verbose=True, info_step=100):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    losses = [0 for i in range(4)]\n",
    "    running_losses = [0 for i in range(4)]\n",
    "    \n",
    "    for i, batch in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_reg, x_features = net(batch['img'].to(device))\n",
    "        x_class = classifier(x_features, batch['target'].to(device))\n",
    "        loss, info = criterion(x_reg, x_class, batch['target'].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        for j in range(len(losses)):\n",
    "            losses[j] += info[j]\n",
    "            running_losses[j] += info[j]\n",
    "        \n",
    "        if (i+1) % info_step == 0:\n",
    "            print(' [{} - {}],\\ttrain loss: {:.5}'.format(epoch+1, i+1, running_loss/info_step/batch_size))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for j in range(len(losses)):\n",
    "                running_losses[j] /= info_step*batch_size\n",
    "            print(' coord loss: {:.5} \\tobj loss: {:.5} \\tclass loss: {:.5} \\tacc: {:.5}'.format(*running_losses))\n",
    "            running_losses = [0 for j in range(4)]\n",
    "            \n",
    "    train_loss /= len(trainset)\n",
    "    \n",
    "    for i in range(len(losses)):\n",
    "        losses[i] /= len(trainset)\n",
    "        \n",
    "    print('\\n [{}], \\ttrain loss: {:.5}'.format(epoch+1, train_loss))\n",
    "    print(' coord loss: {:.5} \\tobj loss: {:.5} \\tclass loss: {:.5} \\tacc: {:.5}'.format(*losses))\n",
    "    return train_loss, losses\n",
    "  \n",
    "\n",
    "def validate():\n",
    "    net.eval()\n",
    "    losses = [0 for i in range(4)]\n",
    "    val_loss = 0.0\n",
    "    for i, batch in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            x_reg, x_features = net(batch['img'].to(device))\n",
    "            x_class = classifier(x_features, batch['target'].to(device))\n",
    "            loss, info  = criterion(x_reg, x_class, batch['target'].to(device))\n",
    "        \n",
    "        val_loss += loss.detach().item()\n",
    "        for j in range(len(losses)):\n",
    "            losses[j] += info[j]\n",
    "            \n",
    "    val_loss /= len(testset)\n",
    "    for i in range(len(losses)):\n",
    "        losses[i] /= len(testset)\n",
    "        \n",
    "    print(' [{}], \\tval loss: {:.5}'.format(epoch+1, val_loss))\n",
    "    print(' coord loss: {:.5} \\tobj loss: {:.5} \\tclass loss: {:.5} \\tacc: {:.5}'.format(*losses))\n",
    "    print()\n",
    "    return val_loss, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1 - 80],\ttrain loss: 57.135\n",
      " coord loss: 0.39881 \tobj loss: 0.36042 \tclass loss: 56.376 \tacc: 0.0125\n",
      " [1 - 160],\ttrain loss: 56.848\n",
      " coord loss: 0.19964 \tobj loss: 0.24367 \tclass loss: 56.404 \tacc: 0.0375\n",
      " [1 - 240],\ttrain loss: 56.543\n",
      " coord loss: 0.11432 \tobj loss: 0.18228 \tclass loss: 56.246 \tacc: 0.075\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3c5d236fffbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-48dc19929581>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(verbose, info_step)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrunning_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/andre6o6/aux/anaconda3/envs/nn/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/andre6o6/aux/anaconda3/envs/nn/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/andre6o6/aux/Projects/VKR/FaceDataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mnp_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m#delete alpha if png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_im\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epoch = 5\n",
    "history = []\n",
    "detailed_history = []\n",
    "\n",
    "net.train()\n",
    "for epoch in range(num_epoch):\n",
    "    scheduler.step()\n",
    "    train_loss, train_info = train(info_step=10)\n",
    "    val_loss, test_info = validate()\n",
    "    history.append((train_loss, val_loss))\n",
    "    detailed_history.append((train_info, test_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou_numpy(box1, box2):\n",
    "    area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    iw = np.minimum(np.expand_dims(box1[2], axis=1), box2[2]) - np.maximum(\n",
    "        np.expand_dims(box1[0], 1), box2[0]\n",
    "    )\n",
    "    ih = np.minimum(np.expand_dims(box1[3], axis=1), box2[3]) - np.maximum(\n",
    "        np.expand_dims(box1[1], 1), box2[1]\n",
    "    )\n",
    "\n",
    "    iw = np.maximum(iw, 0)\n",
    "    ih = np.maximum(ih, 0)\n",
    "\n",
    "    ua = np.expand_dims((box1[2] - box1[0]) * (box1[3] - box1[1]), axis=1) + area - iw * ih\n",
    "\n",
    "    ua = np.maximum(ua, np.finfo(float).eps)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    return intersection / ua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils import non_max_suppression\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=1, shuffle=False)\n",
    "\n",
    "no_face = []\n",
    "wrong = []\n",
    "mae = 0\n",
    "iou = 0\n",
    "acc = 0\n",
    "\n",
    "net.eval()\n",
    "\n",
    "start = time.time()\n",
    "for i,batch in enumerate(testloader):\n",
    "    with torch.no_grad():\n",
    "        output = net(batch['img'].to(device))\n",
    "        \n",
    "        nB,_,nH,nW = output.size()\n",
    "        nC = 285\n",
    "        nA = 3\n",
    "\n",
    "        anchors = criterion.anchors\n",
    "        stride = 1\n",
    "\n",
    "        prediction = output.view(nB, nA, (4+1+nC), nH, nW)         # reshape for convenience\n",
    "        prediction = prediction.permute(0, 1, 3, 4, 2).contiguous()    # Get bbox_attr dimention to be the last\n",
    "\n",
    "        # Get attributes from output tensor\n",
    "        prediction[..., 0] = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "        prediction[..., 1] = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "        prediction[..., 4] = torch.sigmoid(prediction[..., 4])  # Conf\n",
    "        prediction[..., 5:] = torch.softmax(prediction[..., 5:], dim=-1)  # Cls distribution\n",
    "\n",
    "\n",
    "        # Calculate offsets for each grid       \n",
    "        grid_x = torch.arange(nW, dtype=torch.float32).repeat(nW, 1).view([1, 1, nH, nW]).to(device)\n",
    "        grid_y = torch.arange(nH, dtype=torch.float32).repeat(nH, 1).t().view([1, 1, nH, nW]).to(device)\n",
    "        scaled_anchors = torch.FloatTensor([(a_w / stride, a_h / stride) for a_w, a_h in anchors]).to(device)\n",
    "        anchor_w = scaled_anchors[:, 0].view((1, nA, 1, 1))\n",
    "        anchor_h = scaled_anchors[:, 1].view((1, nA, 1, 1))\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        prediction[..., 0] = prediction[..., 0] + grid_x\n",
    "        prediction[..., 1] = prediction[..., 1] + grid_y\n",
    "        prediction[..., 2] = torch.exp(prediction[..., 2]) * anchor_w\n",
    "        prediction[..., 3] = torch.exp(prediction[..., 3]) * anchor_h\n",
    "\n",
    "        prediction = prediction.view(nB, nA*nH*nW, 4+1+nC)\n",
    "        pred = non_max_suppression(prediction, nC)[0]\n",
    "        if pred is None:\n",
    "            no_face.append(i)\n",
    "            continue\n",
    "        \n",
    "        #FIXME\n",
    "        #TODO clamp\n",
    "        pred_boxes = pred[:, :4].cpu().numpy()\n",
    "        scores = pred[:, 4].cpu().numpy()\n",
    "        pred_labels = pred[:, -1].cpu().numpy()\n",
    "\n",
    "        sort_i = np.argsort(scores)\n",
    "        pred_labels = pred_labels[sort_i]\n",
    "        pred_boxes = pred_boxes[sort_i]\n",
    "        \n",
    "        pred_box = pred_boxes[-1]/5\n",
    "        \n",
    "        true_box = batch['target'][0][-1][:4].cpu().numpy()\n",
    "        tb_x1, tb_x2 = true_box[0] - true_box[2] / 2, true_box[0] + true_box[2] / 2\n",
    "        tb_y1, tb_y2 = true_box[1] - true_box[3] / 2, true_box[1] + true_box[3] / 2\n",
    "        true_box = np.array([tb_x1, tb_y1, tb_x2, tb_y2])\n",
    "        \n",
    "        mae += np.sum(np.abs(pred_box-true_box))\n",
    "        iou += bbox_iou_numpy(pred_box, true_box)\n",
    "        \n",
    "        \n",
    "        label = pred_labels[-1]\n",
    "        true_label = batch['target'][0][0][-1].item()\n",
    "        \n",
    "        if label==true_label:\n",
    "            acc+=1\n",
    "        else:\n",
    "            wrong.append(i)\n",
    "            \n",
    "start = time.time() - start\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(no_face)\n",
    "print(wrong)\n",
    "print(\"mae\", mae/len(testset)*320)\n",
    "print(\"iou\", iou/len(testset))\n",
    "print(\"acc\", acc/len(testset))\n",
    "print(\"fps\", len(testset)/start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code\n",
    "net.eval()\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(320),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "filepath = '06.jpg'\n",
    "img = Image.open(filepath)\n",
    "img = transforms.Resize(250)(img)\n",
    "tensor = test_transforms(img).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = net(tensor.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nB,_,nH,nW = output.size()\n",
    "nC = 285\n",
    "nA = 3\n",
    "\n",
    "anchors = criterion.anchors\n",
    "stride = 1\n",
    "\n",
    "prediction = output.view(nB, nA, (4+1+nC), nH, nW)         # reshape for convenience\n",
    "prediction = prediction.permute(0, 1, 3, 4, 2).contiguous()    # Get bbox_attr dimention to be the last\n",
    "\n",
    "# Get attributes from output tensor\n",
    "prediction[..., 0] = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "prediction[..., 1] = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "prediction[..., 4] = torch.sigmoid(prediction[..., 4])  # Conf\n",
    "prediction[..., 5:] = torch.sigmoid(prediction[..., 5:])  # Cls distribution\n",
    "\n",
    "# Calculate offsets for each grid       \n",
    "grid_x = torch.arange(nW, dtype=torch.float32).repeat(nW, 1).view([1, 1, nH, nW]).to(device)\n",
    "grid_y = torch.arange(nH, dtype=torch.float32).repeat(nH, 1).t().view([1, 1, nH, nW]).to(device)\n",
    "scaled_anchors = torch.FloatTensor([(a_w / stride, a_h / stride) for a_w, a_h in anchors]).to(device)\n",
    "anchor_w = scaled_anchors[:, 0].view((1, nA, 1, 1))\n",
    "anchor_h = scaled_anchors[:, 1].view((1, nA, 1, 1))\n",
    "\n",
    "# Add offset and scale with anchors\n",
    "prediction[..., 0] = prediction[..., 0] + grid_x\n",
    "prediction[..., 1] = prediction[..., 1] + grid_y\n",
    "prediction[..., 2] = torch.exp(prediction[..., 2]) * anchor_w\n",
    "prediction[..., 3] = torch.exp(prediction[..., 3]) * anchor_h\n",
    "\n",
    "prediction = prediction.view(nB, nA*nH*nW, 4+1+nC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detections = []\n",
    "\n",
    "#for pred in a:  \n",
    "pred = non_max_suppression(prediction, nC)[0]\n",
    "\n",
    "all_detections.append([np.array([]) for _ in range(nC)])\n",
    "\n",
    "pred_boxes = pred[:, :4].cpu().numpy()\n",
    "scores = pred[:, 4].cpu().numpy()\n",
    "pred_labels = pred[:, -1].cpu().numpy()\n",
    "\n",
    "sort_i = np.argsort(scores)\n",
    "pred_labels = pred_labels[sort_i]\n",
    "pred_boxes = pred_boxes[sort_i]\n",
    "\n",
    "for label in range(nC):\n",
    "    all_detections[-1][label] = pred_boxes[pred_labels == label]\n",
    "    \n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "draw_img = transforms.CenterCrop(640)(img)\n",
    "\n",
    "box = pred_boxes[-1]/5*640\n",
    "\n",
    "draw = ImageDraw.Draw(draw_img)\n",
    "draw.rectangle(box)\n",
    "del draw\n",
    "\n",
    "draw_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
